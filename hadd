To create and run a Hadoop MapReduce program using Python on AWS EMR with files stored in an S3 bucket, follow the steps below. This guide assumes you have AWS CLI installed and configured with the necessary permissions.

### Step-by-Step Guide

#### 1. Prepare Your Python Script

1. **Create the Combined Mapper and Reducer Script**:
   This script will handle both the mapping and reducing phases. It reads the keywords from the S3 bucket and processes the input file accordingly.

```python
# wordcount.py

import sys
import boto3

s3 = boto3.client('s3')

# Replace these with your S3 bucket and key details
BUCKET_NAME = 'your-bucket-name'
KEY_FILE = 'path/to/keys.txt'

def load_keys():
    obj = s3.get_object(Bucket=BUCKET_NAME, Key=KEY_FILE)
    key_content = obj['Body'].read().decode('utf-8')
    keys = set(key_content.strip().split('\n'))
    return keys

def mapper(keys):
    for line in sys.stdin:
        line = line.strip()
        words = line.split()
        for word in words:
            if word in keys:
                print(f"{word}\t1")

def reducer():
    current_word = None
    current_count = 0
    word = None

    for line in sys.stdin:
        line = line.strip()
        word, count = line.split('\t', 1)
        try:
            count = int(count)
        except ValueError:
            continue

        if current_word == word:
            current_count += count
        else:
            if current_word:
                print(f"{current_word}\t{current_count}")
            current_word = word
            current_count = count

    if current_word == word:
        print(f"{current_word}\t{current_count}")

if __name__ == "__main__":
    keys = load_keys()
    if len(sys.argv) > 1 and sys.argv[1] == 'mapper':
        mapper(keys)
    elif len(sys.argv) > 1 and sys.argv[1] == 'reducer':
        reducer()
```

Save this script as `wordcount.py`.

#### 2. Upload Files to S3

Ensure you have the following files:
- `input.txt`: The input text file.
- `keys.txt`: The file containing keywords to count.

Upload these files to your S3 bucket.

```bash
aws s3 cp wordcount.py s3://your-bucket-name/
aws s3 cp input.txt s3://your-bucket-name/path/to/input.txt
aws s3 cp keys.txt s3://your-bucket-name/path/to/keys.txt
```

#### 3. Create an EMR Cluster

Create an EMR cluster via the AWS Management Console or using the AWS CLI. Ensure that your cluster has the necessary permissions to access your S3 bucket.

#### 4. Add the Hadoop Streaming Step to EMR

Create a JSON file (e.g., `wordcount-steps.json`) with the step configuration:

```json
[
  {
    "Type": "CUSTOM_JAR",
    "Name": "WordCount",
    "ActionOnFailure": "CONTINUE",
    "Jar": "command-runner.jar",
    "Args": [
      "hadoop-streaming",
      "-files", "s3://your-bucket-name/wordcount.py,s3://your-bucket-name/keys.txt",
      "-mapper", "python3 wordcount.py mapper",
      "-reducer", "python3 wordcount.py reducer",
      "-input", "s3://your-bucket-name/path/to/input.txt",
      "-output", "s3://your-bucket-name/path/to/output"
    ]
  }
]
```

Replace `your-bucket-name` and other placeholders with your actual S3 bucket and file paths.

Add the step to your EMR cluster using the AWS CLI:

```bash
aws emr add-steps --cluster-id <your-cluster-id> --steps file://wordcount-steps.json
```

Replace `<your-cluster-id>` with your actual EMR cluster ID.

#### 5. Monitor the Job and Retrieve Results

Monitor the job via the EMR console to ensure it completes successfully. Once the job is finished, retrieve the output from the specified S3 path:

```bash
aws s3 cp s3://your-bucket-name/path/to/output/part-00000 - | less
```

This command downloads the output file and displays it using `less`.

### Explanation

1. **Python Script (`wordcount.py`)**:
   - **load_keys()**: Loads the keywords from the S3 bucket.
   - **mapper(keys)**: Processes the input data, emits words from the keys set with a count of 1.
   - **reducer()**: Aggregates counts for each word and outputs the final count.
   - Depending on the argument passed (`mapper` or `reducer`), it runs the appropriate function.

2. **S3 Bucket**:
   - Store `wordcount.py`, `input.txt`, and `keys.txt` in your S3 bucket.

3. **EMR Step Configuration**:
   - Defines the Hadoop streaming job with the mapper and reducer specified as parts of the `wordcount.py` script.

By following these steps, you can create a MapReduce program in Hadoop using Python on AWS EMR, with the necessary files stored in an S3 bucket.
