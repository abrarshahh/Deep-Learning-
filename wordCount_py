To implement a Hadoop WordCount program using Python with the Hadoop Streaming API, follow these steps. The implementation involves creating a mapper script, a reducer script, and running the job using the Hadoop command line.

### Step-by-Step Guide

#### 1. Setup Hadoop Streaming

Ensure you have Hadoop installed and configured properly on your system.

#### 2. Prepare the Mapper and Reducer Scripts

1. **Create the Mapper Script**:
   - This script reads the input line by line, tokenizes each line into words, and outputs each word with a count of 1.

```python
# mapper.py

import sys

# Read keys from a file and store them in a set
keys_file = "keys.txt"
key_words = set()
with open(keys_file, 'r') as f:
    for line in f:
        key_words.add(line.strip())

for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        if word in key_words:
            print(f"{word}\t1")
```

2. **Create the Reducer Script**:
   - This script reads the mapper output, aggregates the counts for each word, and outputs the final count.

```python
# reducer.py

import sys

current_word = None
current_count = 0
word = None

for line in sys.stdin:
    line = line.strip()
    word, count = line.split('\t', 1)
    try:
        count = int(count)
    except ValueError:
        continue

    if current_word == word:
        current_count += count
    else:
        if current_word:
            print(f"{current_word}\t{current_count}")
        current_word = word
        current_count = count

if current_word == word:
    print(f"{current_word}\t{current_count}")
```

#### 3. Prepare Input and Key Files

1. **Create Input File**:
   - Create a text file named `input.txt` with the content you want to process. Example content:
     ```
     Hello world
     Hello Hadoop
     ```

2. **Create Key File**:
   - Create a text file named `keys.txt` containing the keys (words) you want to count. Example content:
     ```
     Hello
     Hadoop
     ```

3. **Place Files in HDFS**:
   - Start your Hadoop cluster.
   - Copy the `input.txt` and `keys.txt` files to the Hadoop Distributed File System (HDFS):

```bash
hadoop fs -mkdir -p /user/<your-username>/wordcount/input
hadoop fs -put /path/to/input.txt /user/<your-username>/wordcount/input
hadoop fs -put /path/to/keys.txt /user/<your-username>/wordcount/input
```

#### 4. Run the Hadoop Streaming Job

Use the Hadoop streaming API to run the MapReduce job, specifying the mapper and reducer scripts.

```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
    -files /path/to/mapper.py,/path/to/reducer.py,/path/to/keys.txt \
    -mapper "python3 mapper.py" \
    -reducer "python3 reducer.py" \
    -input /user/<your-username>/wordcount/input/input.txt \
    -output /user/<your-username>/wordcount/output
```

Replace `/path/to/mapper.py`, `/path/to/reducer.py`, and `/path/to/keys.txt` with the actual paths to your scripts and key file. Ensure the paths in HDFS are correct.

#### 5. Check the Output

After the job completes, view the results in the specified output directory in HDFS:

```bash
hadoop fs -cat /user/<your-username>/wordcount/output/part-00000
```

This will display the word counts for the specified keys.

### Explanation

- **mapper.py**:
  - Reads `keys.txt` to get the list of words to count.
  - Tokenizes input lines and emits each word with a count of 1 if it is in the set of key words.

- **reducer.py**:
  - Aggregates counts for each word and outputs the final count.

By following these steps, you can set up and run a Hadoop WordCount program using Python with Hadoop Streaming.
